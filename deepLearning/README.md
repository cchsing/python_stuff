# Deep Learning Tutorial

1. Activation Function 
    - Rectified Linear Unit (ReLU) - hidden layers 
    - Tanh
    - Sigmoid 
    - Linear
